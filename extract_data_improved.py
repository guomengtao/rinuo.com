#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›ç‰ˆHTMLæ•°æ®æå–è„šæœ¬
ä»free/ç›®å½•ä¸­çš„HTMLæ–‡ä»¶æå–æœåŠ¡ä¿¡æ¯ï¼Œæ›´æ–°åˆ°assets/data/ç›®å½•ä¸­çš„å¯¹åº”JSONæ–‡ä»¶
æ”¯æŒå¤šç§HTMLç»“æ„ï¼šè¡¨æ ¼ã€åˆ—è¡¨é¡¹ã€å¡ç‰‡ç­‰
"""

import os
import re
import json
from bs4 import BeautifulSoup
from pathlib import Path
from datetime import datetime
import html

# åˆ†ç±»åç§°æ˜ å°„ï¼ˆä¸­æ–‡åç§°ï¼‰
CATEGORY_NAMES = {
    'ai': 'äººå·¥æ™ºèƒ½',
    'ai-image': 'AIå›¾åƒç”Ÿæˆ',
    'ai-text': 'AIæ–‡æœ¬å¤„ç†',
    'api': 'APIæœåŠ¡',
    'cdn': 'CDNä¸é™æ€æ‰˜ç®¡',
    'cicd': 'CI/CDå·¥å…·',
    'database': 'æ•°æ®åº“æœåŠ¡',
    'day': 'æ—¥æœŸæ—¶é—´å·¥å…·',
    'devtools': 'å¼€å‘å·¥å…·',
    'domain': 'åŸŸåæœåŠ¡',
    'education': 'æ•™è‚²èµ„æº',
    'email': 'é‚®ä»¶æœåŠ¡',
    'fonts': 'å­—ä½“èµ„æº',
    'free': 'å…è´¹èµ„æºå¯¼èˆª',
    'guide': 'ä½¿ç”¨æŒ‡å—',
    'health': 'å¥åº·åŒ»ç–—',
    'hello': 'Hello World',
    'helloworld': 'Hello World',
    'icons': 'å›¾æ ‡èµ„æº',
    'image': 'å›¾åƒå¤„ç†',
    'index': 'é¦–é¡µ',
    'j-2': 'J-2',
    'jp': 'æ—¥æœ¬',
    'law': 'æ³•å¾‹å·¥å…·',
    'log': 'æ—¥å¿—æœåŠ¡',
    'map': 'åœ°å›¾æœåŠ¡',
    'monitor': 'ç›‘æ§æœåŠ¡',
    'news': 'æ–°é—»èµ„è®¯',
    'ocr': 'OCRè¯†åˆ«',
    'otp': 'OTPéªŒè¯',
    'payment': 'æ”¯ä»˜æœåŠ¡',
    'push': 'æ¨é€æœåŠ¡',
    'serverless': 'æ— æœåŠ¡å™¨',
    'sms': 'çŸ­ä¿¡æœåŠ¡',
    'sport': 'ä½“è‚²æ•°æ®',
    'stock': 'è‚¡ç¥¨æ•°æ®',
    'storage': 'å­˜å‚¨æœåŠ¡',
    'supabase': 'Supabase',
    'test': 'æµ‹è¯•å·¥å…·',
    'translate': 'ç¿»è¯‘æœåŠ¡',
    'video': 'è§†é¢‘å¤„ç†',
    'voice': 'è¯­éŸ³æœåŠ¡',
    'vps': 'VPSæœåŠ¡',
    'weather': 'å¤©æ°”æœåŠ¡',
    'websocket': 'WebSocketæœåŠ¡'
}

def extract_service_data(html_content, category):
    """ä»HTMLå†…å®¹ä¸­æå–æœåŠ¡æ•°æ®"""
    soup = BeautifulSoup(html_content, 'html.parser')
    services = []
    
    # æ–¹æ³•1: æŸ¥æ‰¾è¡¨æ ¼ä¸­çš„æœåŠ¡ä¿¡æ¯
    services.extend(extract_from_tables(soup))
    
    # æ–¹æ³•2: æŸ¥æ‰¾åˆ—è¡¨é¡¹ä¸­çš„æœåŠ¡ä¿¡æ¯
    services.extend(extract_from_list_items(soup))
    
    # æ–¹æ³•3: æŸ¥æ‰¾å¡ç‰‡ä¸­çš„æœåŠ¡ä¿¡æ¯
    services.extend(extract_from_cards(soup))
    
    # å»é‡ï¼ˆåŸºäºtitleå’Œurlï¼‰
    unique_services = []
    seen = set()
    for service in services:
        key = (service.get('title', ''), service.get('url', ''))
        if key not in seen and key[0] and key[1]:
            seen.add(key)
            unique_services.append(service)
    
    return unique_services

def extract_from_tables(soup):
    """ä»è¡¨æ ¼ä¸­æå–æœåŠ¡æ•°æ®"""
    services = []
    tables = soup.find_all('table')
    
    for table in tables:
        rows = table.find_all('tr')
        for row in rows[1:]:  # è·³è¿‡è¡¨å¤´
            cells = row.find_all(['td', 'th'])
            if len(cells) >= 2:
                service_data = {}
                
                # æå–æœåŠ¡åç§°ï¼ˆé€šå¸¸åœ¨ç¬¬ä¸€åˆ—ï¼‰
                if len(cells) > 0:
                    service_name = cells[0].get_text(strip=True)
                    if service_name and service_name not in ['æœåŠ¡åç§°', 'Name', 'å·¥å…·åç§°']:
                        service_data['title'] = service_name
                
                # æå–URLï¼ˆæŸ¥æ‰¾é“¾æ¥ï¼‰
                link = row.find('a')
                if link and link.get('href'):
                    service_data['url'] = link.get('href')
                
                # æå–æè¿°ä¿¡æ¯ï¼ˆé€šå¸¸åœ¨ç¬¬äºŒåˆ—æˆ–ç¬¬ä¸‰åˆ—ï¼‰
                for i in range(1, min(4, len(cells))):
                    cell_text = cells[i].get_text(strip=True)
                    if cell_text and len(cell_text) > 10 and 'å…è´¹' not in cell_text:
                        service_data['description'] = cell_text
                        break
                
                # æå–ç‰¹ç‚¹/é™åˆ¶
                for i in range(1, len(cells)):
                    cell_text = cells[i].get_text(strip=True)
                    if 'é™åˆ¶' in cell_text or 'ç‰¹ç‚¹' in cell_text or 'é€‚ç”¨åœºæ™¯' in cell_text:
                        service_data['features'] = cell_text
                        break
                
                # æå–æ ‡ç­¾
                tags = []
                tag_elements = row.find_all('span', class_=re.compile(r'.*tag.*'))
                for tag in tag_elements:
                    tag_text = tag.get_text(strip=True)
                    if tag_text:
                        tags.append(tag_text)
                
                if tags:
                    service_data['tags'] = tags
                
                # åˆ¤æ–­æ˜¯å¦å…è´¹
                row_text = str(row).lower()
                if 'å®Œå…¨å…è´¹' in row_text or 'å…è´¹' in row_text or 'æ°¸ä¹…å…è´¹' in row_text:
                    service_data['isFree'] = True
                else:
                    service_data['isFree'] = False
                
                # åˆ¤æ–­æ˜¯å¦å¼€æº
                if 'å¼€æº' in row_text or 'open source' in row_text:
                    service_data['isOpenSource'] = True
                else:
                    service_data['isOpenSource'] = False
                
                # ç”ŸæˆID
                if 'title' in service_data:
                    service_data['id'] = service_data['title'].lower().replace(' ', '-').replace('.', '').replace('/', '-')
                
                # è®¾ç½®æ›´æ–°æ—¶é—´
                service_data['updatedAt'] = datetime.now().strftime('%Y-%m-%d')
                
                # è®¾ç½®åœ°åŒºï¼ˆé»˜è®¤ä¸ºglobalï¼‰
                service_data['region'] = 'global'
                
                if service_data.get('title') and service_data.get('url'):
                    services.append(service_data)
    
    return services

def extract_from_list_items(soup):
    """ä»åˆ—è¡¨é¡¹ä¸­æå–æœåŠ¡æ•°æ®"""
    services = []
    
    # æŸ¥æ‰¾å„ç§åˆ—è¡¨é¡¹
    list_items = soup.find_all(['li', 'div'], class_=re.compile(r'.*item.*|.*resource.*'))
    
    for item in list_items:
        service_data = {}
        
        # æå–æ ‡é¢˜
        title_elem = item.find(['h3', 'h4', 'h5', 'a'])
        if title_elem:
            if title_elem.name == 'a':
                service_data['title'] = title_elem.get_text(strip=True)
                service_data['url'] = title_elem.get('href')
            else:
                service_data['title'] = title_elem.get_text(strip=True)
                # åœ¨æ ‡é¢˜å…ƒç´ é™„è¿‘æŸ¥æ‰¾é“¾æ¥
                link = title_elem.find_next('a') or title_elem.find_parent('a')
                if link and link.get('href'):
                    service_data['url'] = link.get('href')
        
        # æå–æè¿°
        desc_elem = item.find('p', class_=re.compile(r'.*text-muted.*|.*muted.*'))
        if desc_elem:
            service_data['description'] = desc_elem.get_text(strip=True)
        
        # æå–æ ‡ç­¾
        tags = []
        tag_elements = item.find_all('span', class_=re.compile(r'.*tag.*|.*pill.*|.*category.*'))
        for tag in tag_elements:
            tag_text = tag.get_text(strip=True)
            if tag_text and len(tag_text) < 20:  # è¿‡æ»¤æ‰å¤ªé•¿çš„æ–‡æœ¬
                tags.append(tag_text)
        
        if tags:
            service_data['tags'] = tags
        
        # åˆ¤æ–­æ˜¯å¦å…è´¹
        item_text = str(item).lower()
        if 'å®Œå…¨å…è´¹' in item_text or 'å…è´¹' in item_text or 'æ°¸ä¹…å…è´¹' in item_text:
            service_data['isFree'] = True
        else:
            service_data['isFree'] = False
        
        # åˆ¤æ–­æ˜¯å¦å¼€æº
        if 'å¼€æº' in item_text or 'open source' in item_text:
            service_data['isOpenSource'] = True
        else:
            service_data['isOpenSource'] = False
        
        # ç”ŸæˆID
        if 'title' in service_data:
            service_data['id'] = service_data['title'].lower().replace(' ', '-').replace('.', '').replace('/', '-')
        
        # è®¾ç½®æ›´æ–°æ—¶é—´
        service_data['updatedAt'] = datetime.now().strftime('%Y-%m-%d')
        
        # è®¾ç½®åœ°åŒºï¼ˆé»˜è®¤ä¸ºglobalï¼‰
        service_data['region'] = 'global'
        
        if service_data.get('title') and service_data.get('url'):
            services.append(service_data)
    
    return services

def extract_from_cards(soup):
    """ä»å¡ç‰‡ä¸­æå–æœåŠ¡æ•°æ®"""
    services = []
    
    # æŸ¥æ‰¾å¡ç‰‡å…ƒç´ 
    cards = soup.find_all('div', class_=re.compile(r'.*card.*'))
    
    for card in cards:
        service_data = {}
        
        # æå–æ ‡é¢˜
        title_elem = card.find(['h5', 'h4', 'h3'])
        if title_elem:
            service_data['title'] = title_elem.get_text(strip=True)
        
        # æå–é“¾æ¥
        link = card.find('a')
        if link and link.get('href'):
            service_data['url'] = link.get('href')
        
        # æå–æè¿°
        desc_elem = card.find('p', class_=re.compile(r'.*text-muted.*'))
        if desc_elem:
            service_data['description'] = desc_elem.get_text(strip=True)
        
        # æå–æ ‡ç­¾
        tags = []
        tag_elements = card.find_all('span', class_=re.compile(r'.*tool-tag.*'))
        for tag in tag_elements:
            tag_text = tag.get_text(strip=True)
            if tag_text:
                tags.append(tag_text)
        
        if tags:
            service_data['tags'] = tags
        
        # åˆ¤æ–­æ˜¯å¦å…è´¹
        card_text = str(card).lower()
        if 'å®Œå…¨å…è´¹' in card_text or 'å…è´¹' in card_text:
            service_data['isFree'] = True
        else:
            service_data['isFree'] = False
        
        # åˆ¤æ–­æ˜¯å¦å¼€æº
        if 'å¼€æº' in card_text or 'open source' in card_text:
            service_data['isOpenSource'] = True
        else:
            service_data['isOpenSource'] = False
        
        # ç”ŸæˆID
        if 'title' in service_data:
            service_data['id'] = service_data['title'].lower().replace(' ', '-').replace('.', '').replace('/', '-')
        
        # è®¾ç½®æ›´æ–°æ—¶é—´
        service_data['updatedAt'] = datetime.now().strftime('%Y-%m-%d')
        
        # è®¾ç½®åœ°åŒºï¼ˆé»˜è®¤ä¸ºglobalï¼‰
        service_data['region'] = 'global'
        
        if service_data.get('title') and service_data.get('url'):
            services.append(service_data)
    
    return services

def update_json_file(json_file_path, services, category):
    """æ›´æ–°JSONæ–‡ä»¶"""
    # è¯»å–ç°æœ‰JSONæ–‡ä»¶
    if os.path.exists(json_file_path):
        with open(json_file_path, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                data = {
                    "schemaVersion": 1,
                    "category": category,
                    "updatedAt": datetime.now().strftime('%Y-%m-%d'),
                    "subcategories": []
                }
    else:
        data = {
            "schemaVersion": 1,
            "category": category,
            "updatedAt": datetime.now().strftime('%Y-%m-%d'),
            "subcategories": []
        }
    
    # æ›´æ–°æ•°æ®
    data['updatedAt'] = datetime.now().strftime('%Y-%m-%d')
    
    # å¦‚æœservicesä¸ä¸ºç©ºï¼Œåˆ›å»ºæˆ–æ›´æ–°subcategories
    if services:
        # æŸ¥æ‰¾ç°æœ‰çš„é€šç”¨åˆ†ç±»ï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»º
        general_category = None
        for subcat in data['subcategories']:
            if subcat.get('slug') == 'general':
                general_category = subcat
                break
        
        if not general_category:
            general_category = {
                "slug": "general",
                "name": "é€šç”¨æœåŠ¡",
                "items": []
            }
            data['subcategories'].append(general_category)
        
        # æ›´æ–°items
        general_category['items'] = services
    
    # å†™å…¥JSONæ–‡ä»¶
    with open(json_file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²æ›´æ–° {json_file_path}")

def create_summary_json(data_dir):
    """åˆ›å»ºæ±‡æ€»JSONæ–‡ä»¶ï¼ŒåŒ…å«æ‰€æœ‰åˆ†ç±»çš„æ•°æ®"""
    summary_file = data_dir / "summary.json"
    
    # æ”¶é›†æ‰€æœ‰JSONæ–‡ä»¶çš„æ•°æ®
    all_services = []
    categories_info = []
    
    json_files = list(data_dir.glob('*.json'))
    
    for json_file in json_files:
        if json_file.name == 'summary.json':
            continue
            
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            category = data.get('category', json_file.stem)
            updated_at = data.get('updatedAt', '')
            
            # è·å–ä¸­æ–‡åˆ†ç±»åç§°
            chinese_name = CATEGORY_NAMES.get(category, category)
            
            # æ”¶é›†åˆ†ç±»ä¿¡æ¯
            categories_info.append({
                "category": category,
                "categoryName": chinese_name,
                "slug": json_file.stem,
                "updatedAt": updated_at,
                "serviceCount": 0
            })
            
            # æ”¶é›†æ‰€æœ‰æœåŠ¡
            for subcat in data.get('subcategories', []):
                for service in subcat.get('items', []):
                    # ä¸ºæ¯ä¸ªæœåŠ¡æ·»åŠ åˆ†ç±»ä¿¡æ¯
                    service_with_category = service.copy()
                    service_with_category['category'] = category
                    service_with_category['categoryName'] = chinese_name
                    service_with_category['categorySlug'] = json_file.stem
                    
                    all_services.append(service_with_category)
                    
                    # æ›´æ–°åˆ†ç±»çš„æœåŠ¡æ•°é‡
                    for cat_info in categories_info:
                        if cat_info['category'] == category:
                            cat_info['serviceCount'] += 1
                            break
                            
        except Exception as e:
            print(f"   âš ï¸  è¯»å– {json_file.name} æ—¶å‡ºé”™: {e}")
    
    # åˆ›å»ºæ±‡æ€»æ•°æ®ç»“æ„
    summary_data = {
        "schemaVersion": 1,
        "type": "summary",
        "updatedAt": datetime.now().strftime('%Y-%m-%d'),
        "totalServices": len(all_services),
        "totalCategories": len(categories_info),
        "categories": categories_info,
        "services": all_services,
        "searchIndex": {
            "byCategory": {},
            "byCategoryName": {},
            "byTag": {},
            "byFreeStatus": {"free": [], "paid": []},
            "byOpenSource": {"openSource": [], "proprietary": []}
        }
    }
    
    # æ„å»ºæœç´¢ç´¢å¼•
    for i, service in enumerate(all_services):
        service_id = i
        
        # æŒ‰åˆ†ç±»ç´¢å¼•
        category = service.get('category', 'unknown')
        if category not in summary_data['searchIndex']['byCategory']:
            summary_data['searchIndex']['byCategory'][category] = []
        summary_data['searchIndex']['byCategory'][category].append(service_id)
        
        # æŒ‰ä¸­æ–‡åˆ†ç±»åç§°ç´¢å¼•
        category_name = service.get('categoryName', 'unknown')
        if category_name not in summary_data['searchIndex']['byCategoryName']:
            summary_data['searchIndex']['byCategoryName'][category_name] = []
        summary_data['searchIndex']['byCategoryName'][category_name].append(service_id)
        
        # æŒ‰æ ‡ç­¾ç´¢å¼•
        tags = service.get('tags', [])
        for tag in tags:
            if tag not in summary_data['searchIndex']['byTag']:
                summary_data['searchIndex']['byTag'][tag] = []
            summary_data['searchIndex']['byTag'][tag].append(service_id)
        
        # æŒ‰å…è´¹çŠ¶æ€ç´¢å¼•
        if service.get('isFree', False):
            summary_data['searchIndex']['byFreeStatus']['free'].append(service_id)
        else:
            summary_data['searchIndex']['byFreeStatus']['paid'].append(service_id)
        
        # æŒ‰å¼€æºçŠ¶æ€ç´¢å¼•
        if service.get('isOpenSource', False):
            summary_data['searchIndex']['byOpenSource']['openSource'].append(service_id)
        else:
            summary_data['searchIndex']['byOpenSource']['proprietary'].append(service_id)
    
    # å†™å…¥æ±‡æ€»æ–‡ä»¶
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²åˆ›å»ºæ±‡æ€»æ–‡ä»¶ {summary_file}")
    print(f"   ğŸ“Š æ€»è®¡ {len(all_services)} ä¸ªæœåŠ¡ï¼Œ{len(categories_info)} ä¸ªåˆ†ç±»")
    
    return summary_data

def main():
    """ä¸»å‡½æ•°"""
    free_dir = Path('free')
    data_dir = Path('assets/data')
    
    if not free_dir.exists():
        print("âŒ freeç›®å½•ä¸å­˜åœ¨")
        return
    
    if not data_dir.exists():
        print("âŒ assets/dataç›®å½•ä¸å­˜åœ¨")
        return
    
    # è·å–æ‰€æœ‰HTMLæ–‡ä»¶
    html_files = list(free_dir.glob('*.html'))
    
    print(f"ğŸ” æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
    
    for html_file in html_files:
        category = html_file.stem  # è·å–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        json_file = data_dir / f"{category}.json"
        
        print(f"\nğŸ“ å¤„ç† {html_file.name}...")
        
        try:
            # è¯»å–HTMLæ–‡ä»¶
            with open(html_file, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            # æå–æœåŠ¡æ•°æ®
            services = extract_service_data(html_content, category)
            
            if services:
                print(f"   ğŸ“Š æå–åˆ° {len(services)} ä¸ªæœåŠ¡")
                # æ›´æ–°JSONæ–‡ä»¶
                update_json_file(json_file, services, category)
            else:
                print(f"   âš ï¸  æœªæå–åˆ°æœåŠ¡æ•°æ®")
                
        except Exception as e:
            print(f"   âŒ å¤„ç† {html_file.name} æ—¶å‡ºé”™: {e}")
    
    # åˆ›å»ºæ±‡æ€»JSONæ–‡ä»¶
    print(f"\nğŸ”— åˆ›å»ºæ±‡æ€»æ–‡ä»¶...")
    create_summary_json(data_dir)
    
    print(f"\nğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼")

if __name__ == '__main__':
    main()
